# Face Recognition Bias Audit  

**Uncovering & Mitigating Bias in AI Models**  

---

## Project Overview  

Artificial Intelligence has the power to transform industries, but with that power comes responsibility.  
This project — **Face Recognition Bias Audit** — was created to investigate, demonstrate, and mitigate **bias in computer vision models**.  

AI systems, especially those trained on datasets such as ImageNet, FairFace, or CelebA, often inherit **biases in gender, ethnicity, and job stereotypes**. These biases can lead to **unfair predictions** and reinforce harmful stereotypes in the real world.  

This repository contains:  

- **Bias Audit Tools** – to analyze how models treat different demographic groups.  
- **Custom CNN Model Training** – on curated datasets with fairness in mind.  
- **Streamlit App** – for interactive exploration of AI bias.  
- **Bias Mitigation Techniques** – including dataset balancing and reweighting.  
- **Educational Documentation** – so anyone, from a beginner to an Ivy League admissions officer, can understand the impact.  

---

## Purpose & Motivation  

The purpose of this project is **not just technical**, but **ethical and educational**:  

- To expose **hidden bias** in widely used AI models.  
- To provide an **interactive platform** where users can experiment and see the effects of bias themselves.  
- To demonstrate **leadership in ethical AI** through research, engineering, and advocacy.  
- To create a **resume-worthy, Ivy League–level project** that blends cutting-edge machine learning with **social responsibility**.  
